<<<<<<< HEAD
import numpy as np
file1 = open("data.txt","r")

laplacian = np.zeros([9,9], dtype=np.int32)
J = np.zeros([9,9], dtype=np.float32)
line = file1.readline()
while line != "":
    lst = line.split()
    ii = int(lst[0])-1
    jj = int(lst[1])-1
    en = float(lst[2])
    laplacian[ii,jj] = 1
    J[ii,jj] = en
    line = file1.readline()

print(laplacian)
print(J)
import pdb;pdb.set_trace()



=======
>>>>>>> b1250baaa20a9bb578d8d052b6ec67bd5aa80232
import numpy             as np
import pandas            as pd
import matplotlib.pyplot as plt

#Declare the array containing the series you want to plot. 
#For example:
time_series_array = np.sin(np.linspace(-np.pi, np.pi, 400)) + np.random.rand((400))
n_steps           = 15 #number of rolling steps for the mean/std.

#Compute curves of interest:
time_series_df = pd.DataFrame(time_series_array)
smooth_path    = time_series_df.rolling(n_steps).mean()
path_deviation = 2 * time_series_df.rolling(n_steps).std()

under_line     = (smooth_path-path_deviation)[0]
over_line      = (smooth_path+path_deviation)[0]

#Plotting:
plt.plot(smooth_path, linewidth=2) #mean curve.
plt.fill_between(path_deviation.index, under_line, over_line, color='b', alpha=.1)
plt.savefig("plot.png")
import pdb;pdb.set_trace()

# import netket as nk
# import numpy as np
# import os
# import argparse
# import time
# import numpy
# import functools

# import netket as nk

# from flowket.operators import NetketOperatorWrapper
# from flowket.utils.jacobian import predictions_jacobian as get_predictions_jacobian
# from flowket.optimizers import ComplexValuesStochasticReconfiguration
# from flowket.optimization import VariationalMonteCarlo, loss_for_energy_minimization
# from flowket.samplers import MetropolisHastingsLocal
# from flowket.callbacks.monte_carlo import TensorBoardWithGeneratorValidationData, \
#     default_wave_function_stats_callbacks_factory
# from flowket.samplers import AutoregressiveSampler

# import tensorflow as tf
# from tensorflow.python.ops.parallel_for import gradients
# from tensorflow.keras.optimizers import SGD, Adam
# import tensorflow.keras.backend as K

# from src.objectives.max_cut import laplacian_to_hamiltonian
# from src.util.helper import evaluate
# from src.util.models import build_model
# from src.util.plottings import plot_graph



# ###############################################################################
# ################################ FlowKet ######################################
# ###############################################################################

# K.set_floatx('float64')

# def run_pyket(cf, laplacian):
#     hilbert_state_shape = (cf.input_size, 1)

#     # build model
#     model, conditional_log_probs_model = build_model(cf)

#     # # build optimizer
#     # if cf.fast_jacobian:
#     #     predictions_jacobian = lambda x: get_predictions_jacobian(keras_model=model)
#     # else:
#     #     predictions_jacobian = lambda x: gradients.jacobian(tf.real(model.output), x, use_pfor=not cf.no_pfor)
#     # if cf.use_stochastic_reconfiguration:
#     #     optimizer = ComplexValuesStochasticReconfiguration(model, predictions_jacobian,
#     #                                                        lr=cf.learning_rate, diag_shift=10.0, 
#     #                                                        iterative_solver=cf.use_iterative,
#     #                                                        use_cholesky=cf.use_cholesky,
#     #                                                        iterative_solver_max_iterations=None)
#     #     model.compile(optimizer=optimizer, loss=loss_for_energy_minimization, metrics=optimizer.metrics)
#     # else:
#     #     optimizer = SGD(lr=cf.learning_rate)
#     #     model.compile(optimizer=optimizer, loss=loss_for_energy_minimization)
#     optimizer = Adam(lr=cf.learning_rate, beta_1=0.9, beta_2=0.999)
#     model.compile(optimizer=optimizer, loss=loss_for_energy_minimization)
    
#     # build objective
#     hamiltonian = laplacian_to_hamiltonian(laplacian)
#     operator = NetketOperatorWrapper(hamiltonian, hilbert_state_shape)

#     # # build sampler
#     # sampler = MetropolisHastingsLocal(model, cf.batch_size,
#     #                                           num_of_chains=cf.pyket_num_of_chains,
#     #                                           unused_sampels=numpy.prod(hilbert_state_shape))
#     # variational_monte_carlo = VariationalMonteCarlo(model, operator, sampler)
#     sampler = AutoregressiveSampler(conditional_log_probs_model, batch_size)
#     variational_monte_carlo = VariationalMonteCarlo(model, operator, sampler)
#     validation_sampler = AutoregressiveSampler(conditional_log_probs_model, batch_size * 16)
#     validation_generator = VariationalMonteCarlo(model, operator, validation_sampler)

#     # set up logger
#     tensorboard = TensorBoardWithGeneratorValidationData(log_dir='tensorboard_logs/try_%s_run_1' % cf.batch_size,
#                                                         generator=variational_monte_carlo, update_freq=1,
#                                                         histogram_freq=1, batch_size=cf.batch_size, write_output=False)
#     callbacks = default_wave_function_stats_callbacks_factory(variational_monte_carlo,
#                                                             validation_generator=validation_generator,
#                                                             true_ground_state_energy=-37) + [tensorboard]

#     model.fit_generator(variational_monte_carlo.to_generator(), steps_per_epoch=5, epochs=1, max_queue_size=0, workers=0)

#     # train
#     start_time = time.time()
#     for i in range(20):
#         # model.fit_generator(variational_monte_carlo.to_generator(), steps_per_epoch=cf.num_of_iterations, epochs=1, callbacks=callbacks,
#         #                     max_queue_size=0, workers=0)
#         model.fit_generator(variational_monte_carlo.to_generator(), steps_per_epoch=20, epochs=1, callbacks=callbacks,
#                             max_queue_size=0, workers=0)
#         evaluate(sampler, operator)

#     end_time = time.time()
#     return end_time - start_time


# ###############################################################################
# ################################ NetKet #######################################
# ###############################################################################
# def run_netket(cf, laplacian):
#     g = nk.graph.Hypercube(length=cf.input_size, n_dim=1)
#     hi = nk.hilbert.Spin(s=0.5, total_sz=0, graph=g)
#     ha = laplacian_to_hamiltonian(laplacian)

#     ma = nk.machine.RbmSpin(alpha=1, hilbert=hi)
#     ma.init_random_parameters(seed=1234, sigma=0.01)
#     sa = nk.sampler.MetropolisLocal(machine=ma)

#     op = nk.optimizer.Sgd(learning_rate=cf.learning_rate)
#     method = 'Sr' if cf.use_stochastic_reconfiguration else 'Gd'

# ##############################################################################
#     # gs = nk.variational.Vmc(
#     #     hamiltonian=ha,
#     #     sampler=sa,
#     #     method=method,
#     #     optimizer=op,
#     #     n_samples=cf.batch_size,
#     #     use_iterative=cf.use_iterative,
#     #     use_cholesky=cf.use_cholesky,
#     #     diag_shift=10.0)
#     gs = nk.variational.Vmc(
#         hamiltonian=ha,
#         sampler=sa,
#         optimizer=op,
#         n_samples=ma.n_par,
#         diag_shift=0.1,
#         use_iterative=True,
#         method='Sr')
# ##############################################################################

#     gs.run(output_prefix="ffnn_test", n_iter=5, save_params_every=5)
#     start_time = time.time()
#     gs.run(output_prefix="ffnn_test", n_iter=cf.num_of_iterations, save_params_every=cf.num_of_iterations)
#     end_time = time.time()
#     return end_time - start_time
# ###############################################################################
# ###############################################################################
# ###############################################################################



# def run(cf):


#     if cf.framework == 'netket':
#         time_in_seconds = run_netket(cf, laplacian)
#     elif cf.framework == 'flowket':
#         if cf.pyket_on_cpu:
#             with tf.device('/cpu:0'):
#                 time_in_seconds = run_pyket(cf, laplacian)
#         else:
#             time_in_seconds = run_pyket(cf, laplacian)
#     else:
#         raise Exception('unknown framework')
#     return time_in_seconds


# if __name__ == '__main__':
#     cf = define_cf_parser().parse_cf()
#     np.random.seed(cf.random_seed)
#     tf.random.set_random_seed(cf.random_seed)

#     time_in_seconds = run(cf)
#     print('finished')
#     print('%s iterations take %s seconds' % (cf.num_of_iterations, time_in_seconds))